# -*- coding: utf-8 -*-
"""predict exam score from study hours

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DSVV5BRe_M8Isew_KdizGGin2l9mDgwb

# 1. Import Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.linear_model import LogisticRegression

""" # 2. Load the Dataset


"""

df = pd.read_csv('NELS.csv')
df.head()

df.info()
df.describe()
df.isnull().sum()  # Check missing values

"""# Data Visualization & Exploration"""

# Histogram for numeric features
df.hist(figsize=(18, 15), bins=30, edgecolor='black')
plt.tight_layout()
plt.show()

plt.figure(figsize=(15,10))
sns.heatmap(df.corr(), cmap="coolwarm", annot=False)
plt.title("Correlation Heatmap")
plt.show()

"""# Feature Engineering & Encoding"""

import pandas as pd
from sklearn.preprocessing import StandardScaler

# --- 1. One-hot encode categorical variables ---
categorical_cols = ["SEX", "RACE", "HSSTAT", "F4HSTYPE"]

# drop_first=True â†’ avoids dummy variable trap
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)

print("âœ… One-hot encoding done. New shape:", df_encoded.shape)

# --- 2. Standardize numerical features ---
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_encoded)

# Convert back to DataFrame for easier handling
df_scaled = pd.DataFrame(scaled_data, columns=df_encoded.columns)

print("âœ… Scaling done. Final dataset shape:", df_scaled.shape)

# Now df_scaled is ready for clustering

target = "F22XMSTD"
# Pairplot with exam score
sns.pairplot(df[[target, "BY2XRSTD", "BY2XMSTD", "BY2XSSTD", "BY2XHSTD"]])
plt.show()

# Boxplot: categorical vs exam score
# Use df_encoded for boxplots as categorical features were one-hot encoded
sns.boxplot(x="SEX_2", y=target, data=df_encoded) # Assuming SEX_1 is one of the encoded columns
plt.show()

sns.boxplot(x="RACE_5", y=target, data=df_encoded) # Assuming RACE_1 is one of the encoded columns
plt.show()

"""#Prediction Model"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# ðŸŽ¯ Step 1: Define target + features
target = "BY2XRSTD"   # replace with your grade column name
X = df.drop(columns=[target])  # features
y = df[target]                # target

# ðŸŽ¯ Step 2: Train/Test Split (80/20 split)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# ðŸŽ¯ Step 3: Train a baseline model (Linear Regression)
lr = LinearRegression()
lr.fit(X_train, y_train)

# ðŸŽ¯ Step 4: Predictions
y_pred = lr.predict(X_test)

# ðŸŽ¯ Step 5: Evaluate model
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print("ðŸ“Š Baseline Model (Linear Regression)")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"RÂ²: {r2:.2f}")

"""### polynomial regression"""

from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Create polynomial regression pipeline (degree=2 as start)
poly_model = Pipeline([
    ("poly_features", PolynomialFeatures(degree=2, include_bias=False)),
    ("lin_reg", LinearRegression())
])

# Fit model
poly_model.fit(X_train, y_train)

# Predictions
y_pred_poly = poly_model.predict(X_test)

# Evaluation
mae = mean_absolute_error(y_test, y_pred_poly)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_poly))
r2 = r2_score(y_test, y_pred_poly)

print("Polynomial Regression (degree=3)")
print("MAE:", mae)
print("RMSE:", rmse)
print("RÂ²:", r2)

